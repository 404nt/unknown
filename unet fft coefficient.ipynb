{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb26a466-944f-4beb-9e36-698116118f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "ngpu= 1\n",
    "print(torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\"))\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c98bf21-348a-4675-ae93-6c7f88aa92bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from complexPyTorch.complexLayers import ComplexConv2d, ComplexMaxPool2d, ComplexReLU\n",
    "from complexPyTorch.complexFunctions import complex_relu, complex_max_pool2d, complex_upsample\n",
    "\n",
    "# class ComplexUpsample(Module):\n",
    "\n",
    "#     def __init__(self,kernel_size, stride= None, padding = 0,\n",
    "#                  dilation = 1, return_indices = False, ceil_mode = False):\n",
    "#         super(ComplexMaxPool2d,self).__init__()\n",
    "#         self.kernel_size = kernel_size\n",
    "#         self.stride = stride\n",
    "#         self.padding = padding\n",
    "#         self.dilation = dilation\n",
    "#         self.ceil_mode = ceil_mode\n",
    "#         self.return_indices = return_indices\n",
    "\n",
    "#     def forward(self,input):\n",
    "#         return complex_max_pool2d(input,kernel_size = self.kernel_size,\n",
    "#                                 stride = self.stride, padding = self.padding,\n",
    "#                                 dilation = self.dilation, ceil_mode = self.ceil_mode,\n",
    "#                                 return_indices = self.return_indices)\n",
    "\n",
    "\n",
    "def double_conv(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        ComplexConv2d(in_channels, out_channels, 3, padding=1),\n",
    "        ComplexReLU(),\n",
    "        ComplexConv2d(out_channels, out_channels, 3, padding=1),\n",
    "        ComplexReLU()\n",
    "    )\n",
    "\n",
    "def coe_to_spatial(img):\n",
    "    iimage = torch.fft.ifft2(img)\n",
    "    iimage = torch.abs(iimage)\n",
    "    \n",
    "    iimage -= torch.min(iimage)\n",
    "    iimage /= torch.max(iimage)\n",
    "    return iimage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0dad4b-0af8-4dd0-9f19-2e7d12c498db",
   "metadata": {},
   "source": [
    "## main model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce900511-3800-4d54-ba83-0958fdecc413",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "        self.dconv_down1 = double_conv(3, 32)\n",
    "        self.dconv_down2 = double_conv(32, 64)\n",
    "        self.dconv_down3 = double_conv(64, 128)\n",
    "\n",
    "        self.dconv_down4 = double_conv(128, 256)\n",
    "   \n",
    "        self.dconv_up3 = double_conv(128 + 256, 128)\n",
    "        self.dconv_up2 = double_conv(64 + 128, 64)\n",
    "        self.dconv_up1 = double_conv(32 + 64, 32)\n",
    "\n",
    "        self.conv_last = ComplexConv2d(32, n_class, 1)\n",
    "        self.conv_llast = ComplexConv2d(3, 1, 1)\n",
    "        self.maxpool = ComplexMaxPool2d(2)\n",
    "        # self.upsample = complex_upsample(scale_factor=2, align_corners=True)\n",
    "        '''\n",
    "        maxpool用於下採樣\n",
    "        upsample用於上採樣\n",
    "        '''\n",
    "    def forward(self, x):\n",
    "        conv1 = self.dconv_down1(x)\n",
    "        x = self.maxpool(conv1)\n",
    "\n",
    "        conv2 = self.dconv_down2(x)\n",
    "        x = self.maxpool(conv2)\n",
    "        \n",
    "        conv3 = self.dconv_down3(x)\n",
    "        x = self.maxpool(conv3)   \n",
    "        \n",
    "        x = self.dconv_down4(x)\n",
    "        \n",
    "        x = complex_upsample(x, scale_factor=2, mode='bilinear', align_corners=True)   \n",
    "        x = torch.cat([x, conv3], dim=1)\n",
    "        \n",
    "        x = self.dconv_up3(x)\n",
    "        x = complex_upsample(x, scale_factor=2, mode='bilinear', align_corners=True)     \n",
    "        x = torch.cat([x, conv2], dim=1)       \n",
    "\n",
    "        x = self.dconv_up2(x)\n",
    "        x = complex_upsample(x, scale_factor=2, mode='bilinear', align_corners=True)       \n",
    "        x = torch.cat([x, conv1], dim=1)   \n",
    "        \n",
    "        x = self.dconv_up1(x)\n",
    "        \n",
    "        out = self.conv_last(x)\n",
    "        return out\n",
    "\n",
    "def weights_init(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if classname.find('Linear') != -1:\n",
    "            # apply a uniform distribution to the weights and a bias=0\n",
    "            m.weight.data.uniform_(0.0, 1.0)\n",
    "            m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fe195e-8529-48e5-9099-19e6275f4733",
   "metadata": {},
   "source": [
    "<p>繪製model:<p>\n",
    "\n",
    "<pre><code>from torchviz import make_dot\n",
    "x = torch.randn(1, 3, 256, 256).requires_grad_(True).cuda()\n",
    "y = model(x)\n",
    "vis_graph = make_dot(y, params=dict(list(model.named_parameters()) + [('x', x)]))\n",
    "vis_graph.view()\n",
    "</code></pre>\n",
    "\n",
    "<p>輸出 Model parameters:\n",
    "<pre><code> \n",
    "model = UNet(n_class=3).cuda()\n",
    "for name, param in model.named_parameters():\n",
    "     print(param)\n",
    "</code></pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e68ab6f-8967-4e17-a376-944aff04ccd6",
   "metadata": {},
   "source": [
    "## training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa32b732-844a-454b-b2c9-6dc7f739cde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "def mse_loss(output, target):\n",
    "    return torch.mean((output - target)**2)\n",
    "\n",
    "def train_model(model, optimizer, scheduler, num_epochs=25):\n",
    "    \n",
    "    log_path = os.path.join(os.getcwd(), \"log/log-complex\")\n",
    "    if not os.path.exists(log_path):\n",
    "        os.makedirs(log_path)\n",
    "        \n",
    "    net_path = os.path.join(os.getcwd(), \"net/net-complex\")\n",
    "    if not os.path.exists(net_path):\n",
    "        os.makedirs(net_path)\n",
    "    \n",
    "    writer = SummaryWriter(log_path)\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 1e10\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        for param_group in optimizer.param_groups:\n",
    "            print('LR', param_group['lr'])\n",
    "            \n",
    "        since = time.time()\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        \n",
    "        train_loss = []\n",
    "        epoch_samples = 0\n",
    "        with torch.set_grad_enabled(True):\n",
    "            for (image, label) in tqdm(dataloaders['train']):\n",
    "                # print('images, labels', images, labels)\n",
    "                image, label = image.to(device), label.to(device)\n",
    "                images, labels = image.to(torch.cfloat), label.to(torch.cfloat)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                outputs = model(images)\n",
    "                \n",
    "                loss = mse_loss(outputs, labels)\n",
    "\n",
    "                train_loss.append(loss.data.cpu().numpy() * images.size(0))\n",
    "            \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # scheduler.step()\n",
    "\n",
    "                epoch_samples += images.size(0)\n",
    "            \n",
    "            print('training: total loss: {:.6f}'.format(sum(train_loss) / epoch_samples))\n",
    "            # writer.add_scalar('Training/Loss', sum(train_loss) / epoch_samples, epoch)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = []\n",
    "        with torch.no_grad():\n",
    "            epoch_samples = 0\n",
    "            for (image, label) in tqdm(dataloaders['val']):\n",
    "                image, label = image.to(device), label.to(device)\n",
    "                images, labels = image.to(torch.cfloat), label.to(torch.cfloat)\n",
    "\n",
    "                outputs = model(images)\n",
    "                metrics = defaultdict(float)\n",
    "                \n",
    "                loss = mse_loss(outputs, labels)\n",
    "\n",
    "                val_loss.append(loss.data.cpu().numpy() * images.size(0))\n",
    "                \n",
    "                epoch_samples += images.size(0)\n",
    "                \n",
    "            print('validation: total loss: {:.6f}'.format(sum(val_loss) / epoch_samples))\n",
    "            # writer.add_scalar('Validation/Loss', sum(val_loss) / epoch_samples, epoch)\n",
    "            epoch_loss = sum(val_loss) / epoch_samples\n",
    "\n",
    "                # deep copy the model\n",
    "            \n",
    "            \n",
    "            torch.save(model, net_path + '/net-' + str(epoch+1) + '.pkl')\n",
    "            if  epoch_loss < best_loss:\n",
    "                print(\"saving best model\")\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    print('Best val loss: {:4f}'.format(best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c073cf1e-63f4-4fe2-929f-0570cc7f5893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import copy\n",
    "from dataloader import get_train_augmentation, get_test_augmentation, get_loader\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "import os\n",
    "\n",
    "tr_img_folder = os.path.join(r'C:\\Users\\user\\pythonProject\\mission87\\data\\DUTS\\DUTS-TR\\DUTS-TR-Image')\n",
    "tr_gt_folder = os.path.join(r'C:\\Users\\user\\pythonProject\\mission87\\data\\DUTS\\DUTS-TR\\DUTS-TR-Mask')\n",
    "\n",
    "ver = 2\n",
    "train_transform = get_train_augmentation(img_size=224, ver=ver)\n",
    "test_transform = get_test_augmentation(img_size=224)\n",
    "\n",
    "train_loader = get_loader(tr_img_folder, tr_gt_folder, phase='train',\n",
    "                          batch_size=2, shuffle=True, num_workers=0,\n",
    "                          transform=train_transform)\n",
    "val_loader = get_loader(tr_img_folder, tr_gt_folder, phase='val',\n",
    "                        batch_size=2, shuffle=False, num_workers=0,\n",
    "                        transform=train_transform)\n",
    "\n",
    "dataloaders = {\n",
    "    'train': train_loader,\n",
    "    'val': val_loader\n",
    "}\n",
    "\n",
    "model = UNet(n_class=3).cuda()\n",
    "model.apply(weights_init)\n",
    "optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=5, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b159bc8b-f2cc-4ee5-a551-7c7bd429463a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## model test complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4c8cad-a542-468b-abd8-e32784da0bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.randn([1, 3, 224, 224])\n",
    "test = test.to(torch.cfloat) #dtype to complex64\n",
    "test = test.cuda()\n",
    "\n",
    "pred = model(test)\n",
    "loss = mse_loss(test, pred)\n",
    "print(loss)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4b57d3-74a9-4473-b694-94fd750284e3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## check training/valid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03859ba8-3274-43fd-9a29-7fe468b3fe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "image, mask = next(iter(train_loader))\n",
    "\n",
    "s = {1: 'Nothing change', 2: 'Fourier coefficients', 3: 'Fourier amplitude', 4: 'Fourier phase'}\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "fig.suptitle(s[ver], verticalalignment='bottom')\n",
    "ax1.imshow(coe_to_spatial(image[0].to(torch.cfloat)).permute(1, 2, 0))\n",
    "ax1.set_title('image')\n",
    "ax2.imshow(coe_to_spatial(mask[0].to(torch.cfloat)).permute(1, 2, 0), 'gray')\n",
    "ax2.set_title('mask')\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.99)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde0b8a2-8301-46f0-b8b9-b4010ee5db33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(output, target):\n",
    "    loss = torch.mean((output - target)**2)\n",
    "    return loss\n",
    "\n",
    "image = image.to(torch.cfloat).cuda()\n",
    "mask = mask.cuda()\n",
    "pred = model(image)\n",
    "mse_loss(pred, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f56643-155a-436d-a694-051e8d3427d1",
   "metadata": {},
   "source": [
    "# Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19aad67-cfe3-4cc3-b734-8342cc7792f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(model, optimizer_ft, exp_lr_scheduler, num_epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d54eb53-4772-4976-88e7-a582a18183ea",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb40cb6-3c91-4cf8-a3d4-2325903855a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "image, mask = image.cuda(), mask.cuda()\n",
    "pred = model(image)\n",
    "pred, pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12c7b91-e554-48f2-b26e-531894f30ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fshift = torch.fft.fftshift(image[0])\n",
    "\n",
    "ifshift = torch.fft.ifftshift(fshift)\n",
    "iimage = torch.fft.ifft2(ifshift)\n",
    "iimage = torch.abs(iimage.permute(1, 2, 0))\n",
    "\n",
    "iimage -= torch.min(iimage)\n",
    "iimage /= torch.max(iimage)\n",
    "\n",
    "plt.imshow(iimage.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a49b59-a15e-462c-aad2-0a5a93a4d30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fshift = torch.fft.fftshift(mask[0])\n",
    "\n",
    "ifshift = torch.fft.ifftshift(fshift)\n",
    "iimage = torch.fft.ifft2(ifshift)\n",
    "iimage = torch.abs(iimage.permute(1, 2, 0))\n",
    "\n",
    "iimage -= torch.min(iimage)\n",
    "iimage /= torch.max(iimage)\n",
    "\n",
    "plt.imshow(iimage.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ead983-cb32-4989-9c09-cb3790c3868c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fshift = torch.fft.fftshift(pred[0])\n",
    "\n",
    "ifshift = torch.fft.ifftshift(fshift)\n",
    "iimage = torch.fft.ifft2(ifshift)\n",
    "iimage = torch.abs(iimage.permute(1, 2, 0))\n",
    "\n",
    "iimage -= torch.min(iimage)\n",
    "iimage /= torch.max(iimage)\n",
    "\n",
    "plt.imshow(iimage.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6656ce43-a11f-47df-920f-79c3d721452c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1.8",
   "language": "python",
   "name": "torch1.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
